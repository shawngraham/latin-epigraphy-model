{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ok, used my latinepi (which needs cleaning up) to download *a lot* of inscriptions from edh. Then used a combination of scripts & llm to annotate them. In this notebook, we clean out the unsuccessfully annotated inscriptions, leaving us with a core of real data, and then we generate a whole bunch of synthetic inscriptions (thank god for formulaic epigraphy, eh?) that are correctly annotated, and mix them both together. The goal is to train the latinCy spaCy model to recognize the elements of funerary inscriptions, for data extraction from transcriptions. Why not?"
      ],
      "metadata": {
        "id": "rEmjAaQYVHdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ufyr6JarIeFz"
      },
      "outputs": [],
      "source": [
        "!mkdir assets         # To store your raw data files (jsonl, csv)\n",
        "!mkdir configs        # To store configuration files\n",
        "!mkdir scripts        # To store helper scripts (like data conversion)\n",
        "!mkdir training       # To store the output of the training process\n",
        "!mkdir corpus         # To store the processed .spacy files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U spacy #already in colab\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!pip install \"la-core-web-sm @ https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\"\n",
        "!pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\"\n",
        "#\n",
        "# this is what we're going to retrain.\n",
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7rqCltXoIgt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then you have to run this. It will say things have crashed. Ignore and continue.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1vtGdT_tIlE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean up some real inscriptions to add with the synthetic ones"
      ],
      "metadata": {
        "id": "reytWfT4SkkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### ok, try to clean up some real ones and then mix them in with the synthetic\n",
        "\n",
        "## this is a couple hundred rows of real inscriptions that were annotated\n",
        "## through combination of scripts & llm, but the results had issues with\n",
        "## annotation offsets-->\n",
        "!wget https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl -O assets/to-clean.jsonl\n",
        "\n",
        "# so, the 'output.jsonl' here is the result of the script at the bottom of the page that cleans up 753 rows of data from edh using this: https://github.com/shawngraham/latinepi/blob/main/edh-real-data-to-jsonl.py; the to-clean.jsonl is from another 1100 rows of data from edh (and there is a chance that perhaps there are duplicate rows, that's on my list of things to check/fix). Both were annotated using regex & dictionaries, but the first 753 made that a lot easier because I could use the column metadata for it (the second 1100 I obtained using my little command line thing latinepi for downloading from edh. Look, I'm making it all up as I go along.)-->\n",
        "!wget https://gist.githubusercontent.com/shawngraham/8ed8e45daf57dc3b0425441d008005fd/raw/a5039a138e40ea1faf4485bec58518648266957f/edh-753.jsonl -O assets/output.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-9PwnTCNCRf",
        "outputId": "9888b12c-b345-45e9-9221-c30bdec0ff05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-23 01:30:44--  https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459000 (448K) [text/plain]\n",
            "Saving to: ‘assets/to-clean.jsonl’\n",
            "\n",
            "assets/to-clean.jso 100%[===================>] 448.24K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-23 01:30:45 (3.40 MB/s) - ‘assets/to-clean.jsonl’ saved [459000/459000]\n",
            "\n",
            "--2025-11-23 01:30:45--  https://gist.githubusercontent.com/shawngraham/8ed8e45daf57dc3b0425441d008005fd/raw/a5039a138e40ea1faf4485bec58518648266957f/edh-753.jsonl\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 315651 (308K) [text/plain]\n",
            "Saving to: ‘assets/output.jsonl’\n",
            "\n",
            "assets/output.jsonl 100%[===================>] 308.25K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-23 01:30:46 (2.98 MB/s) - ‘assets/output.jsonl’ saved [315651/315651]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cat assets/output.jsonl assets/to-clean.jsonl > assets/full_real_data.jsonl"
      ],
      "metadata": {
        "id": "RhZf8bgPfi5G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## so we clean out the ones that are no good, just keeps the cleanest/easiest to fix\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def clean_real_inscriptions(input_path, output_path, model='la_core_web_lg'):\n",
        "    \"\"\"\n",
        "    Cleans real inscription data:\n",
        "    1. Copies transcription → text\n",
        "    2. Re-validates all annotation spans\n",
        "    3. Drops records with errors or unalignable annotations\n",
        "    4. Outputs clean subset ready for training\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(model)\n",
        "\n",
        "    stats = {\n",
        "        \"total\": 0,\n",
        "        \"has_error_flag\": 0,\n",
        "        \"no_transcription\": 0,\n",
        "        \"no_annotations\": 0,\n",
        "        \"perfect_match\": 0,\n",
        "        \"fixed_spans\": 0,\n",
        "        \"dropped_records\": 0,\n",
        "        \"saved\": 0\n",
        "    }\n",
        "\n",
        "    salvaged = []\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Skip flagged errors\n",
        "            if record.get(\"_error\"):\n",
        "                stats[\"has_error_flag\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Use transcription as text\n",
        "            text = record.get('transcription', '').strip()\n",
        "            if not text:\n",
        "                stats[\"no_transcription\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Update record to use text field\n",
        "            record['text'] = text\n",
        "            record.pop('transcription', None)  # Remove redundant field\n",
        "\n",
        "            # Skip if no annotations\n",
        "            annotations = record.get('annotations', [])\n",
        "            if not isinstance(annotations, list) or not annotations:\n",
        "                stats[\"no_annotations\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Validate/fix annotations\n",
        "            doc = nlp.make_doc(text)\n",
        "            validated_ents = []\n",
        "            record_is_salvageable = True\n",
        "\n",
        "            for entity in annotations:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "\n",
        "                # Validate span is within text bounds\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "                # Check if span matches actual text\n",
        "                span_text = text[start:end]\n",
        "\n",
        "                # Try alignment\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is not None:\n",
        "                    # Span aligned successfully\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        # Perfect match\n",
        "                        validated_ents.append([start, end, label])\n",
        "                        stats[\"perfect_match\"] += 1\n",
        "                    else:\n",
        "                        # Adjusted but acceptable\n",
        "                        validated_ents.append([span.start_char, span.end_char, label])\n",
        "                        stats[\"fixed_spans\"] += 1\n",
        "                else:\n",
        "                    # Could not align - record is bad\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "            if record_is_salvageable and validated_ents:\n",
        "                record['annotations'] = validated_ents\n",
        "                salvaged.append(record)\n",
        "                stats[\"saved\"] += 1\n",
        "            else:\n",
        "                stats[\"dropped_records\"] += 1\n",
        "\n",
        "    # Save cleaned data\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for record in salvaged:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Real inscription cleaning complete\")\n",
        "    print(f\"   Total records: {stats['total']}\")\n",
        "    print(f\"   Flagged as error: {stats['has_error_flag']}\")\n",
        "    print(f\"   No transcription: {stats['no_transcription']}\")\n",
        "    print(f\"   No annotations: {stats['no_annotations']}\")\n",
        "    print(f\"   Perfect annotations: {stats['perfect_match']}\")\n",
        "    print(f\"   Fixed spans: {stats['fixed_spans']}\")\n",
        "    print(f\"   Dropped (unrecoverable): {stats['dropped_records']}\")\n",
        "    print(f\"   ✅ SAVED: {stats['saved']} clean records\")\n",
        "    print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Run it\n",
        "#clean_real_inscriptions('assets/to-clean.jsonl', 'assets/real_inscriptions_clean.jsonl')\n",
        "clean_real_inscriptions('assets/full_real_data.jsonl', 'assets/real_inscriptions_clean.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Va11flNKvj",
        "outputId": "bac93cac-98df-4091-ea31-955294cba469"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Real inscription cleaning complete\n",
            "   Total records: 1853\n",
            "   Flagged as error: 221\n",
            "   No transcription: 0\n",
            "   No annotations: 25\n",
            "   Perfect annotations: 5022\n",
            "   Fixed spans: 8358\n",
            "   Dropped (unrecoverable): 175\n",
            "   ✅ SAVED: 1432 clean records\n",
            "   Saved to: assets/real_inscriptions_clean.jsonl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total': 1853,\n",
              " 'has_error_flag': 221,\n",
              " 'no_transcription': 0,\n",
              " 'no_annotations': 25,\n",
              " 'perfect_match': 5022,\n",
              " 'fixed_spans': 8358,\n",
              " 'dropped_records': 175,\n",
              " 'saved': 1432}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make some synthetic"
      ],
      "metadata": {
        "id": "bBRXbtgyTcG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced Synthetic Latin Inscription Generator (Final v4).\n",
        "Features:\n",
        "- Roman Numerals for Ages (Standard Epigraphic format).\n",
        "- Correct Case Agreement (Nominative/Dative for Praenomina).\n",
        "- 'Force Military' mode for targeted training data.\n",
        "- Aggressive filtering of real data (removing ghost words, gender clashes).\n",
        "- Smart inflection and abbreviation logic.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# --- LATIN UTILITIES ---\n",
        "\n",
        "def to_roman(n):\n",
        "    \"\"\"Convert integer to Roman numeral.\"\"\"\n",
        "    if not isinstance(n, int) or n <= 0: return str(n)\n",
        "    val = [\n",
        "        100, 90, 50, 40,\n",
        "        10, 9, 5, 4,\n",
        "        1\n",
        "    ]\n",
        "    syb = [\n",
        "        \"C\", \"XC\", \"L\", \"XL\",\n",
        "        \"X\", \"IX\", \"V\", \"IV\",\n",
        "        \"I\"\n",
        "    ]\n",
        "    roman_num = ''\n",
        "    i = 0\n",
        "    while  n > 0:\n",
        "        for _ in range(n // val[i]):\n",
        "            roman_num += syb[i]\n",
        "            n -= val[i]\n",
        "        i += 1\n",
        "    return roman_num\n",
        "\n",
        "# Map for common Praenomina (Nominative -> Dative)\n",
        "PRAENOMEN_DAT_MAP = {\n",
        "    'M.': 'M.', 'L.': 'L.', 'C.': 'C.', 'T.': 'T.', 'Q.': 'Q.', # Abbrevs don't change\n",
        "    'Lucius': 'Lucio', 'Marcus': 'Marco', 'Quintus': 'Quinto',\n",
        "    'Caius': 'Caio', 'Gaius': 'Gaio', 'Titus': 'Tito',\n",
        "    'Publius': 'Publio', 'Sextus': 'Sexto', 'Aulus': 'Aulo',\n",
        "    'Decimus': 'Decimo', 'Gnaeus': 'Gnaeo', 'Spurius': 'Spurio',\n",
        "    'Tiberius': 'Tiberio', 'Manius': 'Manio', 'Servius': 'Servio',\n",
        "    'Appius': 'Appio', 'Numerius': 'Numerio'\n",
        "}\n",
        "\n",
        "def inflect_praenomen(name, case='dative'):\n",
        "    \"\"\"Specific lookup for Praenomina case agreement.\"\"\"\n",
        "    if case == 'dative':\n",
        "        return PRAENOMEN_DAT_MAP.get(name, name)\n",
        "    return name\n",
        "\n",
        "def inflect_latin(word, case='dative'):\n",
        "    \"\"\"\n",
        "    Heuristic to decline Latin names.\n",
        "    Prevents double-inflection and handles common suffixes.\n",
        "    \"\"\"\n",
        "    word = word.strip()\n",
        "    if not word or word.endswith('.'): return word\n",
        "\n",
        "    if case == 'dative':\n",
        "        # 2nd Declension (Marcus -> Marco)\n",
        "        if word.endswith('us'): return word[:-2] + 'o'\n",
        "        if word.endswith('ius'): return word[:-3] + 'io'\n",
        "\n",
        "        # 1st Declension (Iulia -> Iuliae)\n",
        "        if word.endswith('a'): return word + 'e'\n",
        "\n",
        "        # 2nd Declension -er (Afer -> Afro, Niger -> Nigro)\n",
        "        # Replaces the 'er' with 'ro' to avoid 'Celerro'\n",
        "        if word.endswith('er'): return word[:-2] + 'ro'\n",
        "\n",
        "        # 3rd Declension (Felix -> Felici)\n",
        "        if word.endswith('ex'): return word[:-2] + 'ici'\n",
        "\n",
        "        # Safety: If it already looks Dative/Genitive (ends in o or i), leave it.\n",
        "        if word.endswith('o') or word.endswith('i'): return word\n",
        "\n",
        "    return word\n",
        "\n",
        "def abbreviate_text(word, p=0.7):\n",
        "    \"\"\"Randomly abbreviate common terms.\"\"\"\n",
        "    lookup = {\n",
        "        'filius': 'f.', 'filia': 'f.', 'vixit': 'v.', 'annos': 'an.',\n",
        "        'annis': 'an.', 'mensibus': 'm.', 'diebus': 'd.',\n",
        "        'bene merenti': 'b. m.', 'hic situs est': 'h. s. e.',\n",
        "        'miles': 'mil.', 'legio': 'leg.', 'frater': 'fr.',\n",
        "        'coniunx': 'con.', 'posuit': 'p.', 'fecit': 'f.',\n",
        "        'heres': 'h.', 'libertus': 'lib.', 'optio': 'opt.',\n",
        "        'centurio': '7.', # Common epigraphic symbol\n",
        "        'praefectus': 'praef.'\n",
        "    }\n",
        "\n",
        "    # 1. Check dictionary\n",
        "    if word.lower() in lookup and random.random() < p:\n",
        "        return lookup[word.lower()]\n",
        "\n",
        "    # 2. Heuristic abbreviation (truncate at 3-4 chars)\n",
        "    if len(word) > 4 and random.random() < 0.2:\n",
        "        return word[:3] + '.'\n",
        "\n",
        "    return word\n",
        "\n",
        "# --- FREQUENCY EXTRACTOR ---\n",
        "\n",
        "class FrequencyExtractor:\n",
        "    @staticmethod\n",
        "    def extract_frequencies(input_path):\n",
        "        counters = {k: Counter() for k in ['PRAENOMEN', 'NOMEN', 'COGNOMEN', 'TRIBUS', 'ORIGO', 'OCCUPATION']}\n",
        "\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    text = data.get('text', '') or data.get('transcription', '')\n",
        "                    for ann in data.get('annotations', []):\n",
        "                        if len(ann) == 3:\n",
        "                            s, e, lbl = ann\n",
        "                            lbl = lbl.upper()\n",
        "                            if lbl == \"TRIBE\": lbl = \"TRIBUS\"\n",
        "                            if lbl == \"ORIGIN\": lbl = \"ORIGO\"\n",
        "\n",
        "                            if lbl in counters:\n",
        "                                snippet = text[s:e].strip()\n",
        "\n",
        "                                # FIX 1: Handle CamelCase errors in OCR (ForoIuli -> Foro Iuli)\n",
        "                                snippet = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', snippet)\n",
        "\n",
        "                                # Clean punctuation\n",
        "                                clean_snip = re.sub(r'[^\\w\\.]', '', snippet)\n",
        "\n",
        "                                # FIX 2: Filter garbage (< 3 chars), but allow abbrevs like \"T.\"\n",
        "                                if len(clean_snip) < 3 and not clean_snip.endswith('.'):\n",
        "                                    continue\n",
        "\n",
        "                                counters[lbl][clean_snip] += 1\n",
        "                except: continue\n",
        "        return counters\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weighted_list(counter, top_n=50):\n",
        "        if not counter: return []\n",
        "        population = [word for word, count in counter.most_common(top_n)]\n",
        "        weights = [count for word, count in counter.most_common(top_n)]\n",
        "        return random.choices(population, weights=weights, k=1000)\n",
        "\n",
        "# --- GENERATOR ---\n",
        "\n",
        "class InscriptionGenerator:\n",
        "    def __init__(self, data_pools=None):\n",
        "        self.pools = data_pools if data_pools else {}\n",
        "\n",
        "        # --- FIX 3: SANITIZE POOLS ---\n",
        "        BAD_OCCUPATIONS = {'colonia', 'municipium', 'vixit', 'annos', 'fecit', 'patrono',\n",
        "                           'filio', 'coniugi', 'fratri', 'sorori', 'gustin', 'flamin', 'uxor'}\n",
        "\n",
        "        if self.pools:\n",
        "            # Filter Names: Keep mostly Masculine Nominative (-us, -o, -x, -er)\n",
        "            for key in ['NOMEN', 'COGNOMEN']:\n",
        "                self.pools[key] = [\n",
        "                    x for x in self.pools[key]\n",
        "                    if len(x) > 3 and (x.endswith('us') or x.endswith('o') or x.endswith('x') or x.endswith('er') or x.endswith('i'))\n",
        "                ]\n",
        "\n",
        "            # Filter Occupations\n",
        "            if 'OCCUPATION' in self.pools:\n",
        "                self.pools['OCCUPATION'] = [\n",
        "                    x for x in self.pools['OCCUPATION']\n",
        "                    if len(x) > 3 and x.lower() not in BAD_OCCUPATIONS\n",
        "                ]\n",
        "\n",
        "        # --- DEFAULTS ---\n",
        "        defaults = {\n",
        "            'PRAENOMEN': ['M', 'L', 'C', 'T', 'Q', 'Sex', 'Ti', 'Cn', 'Sp', 'App', 'D'],\n",
        "            'NOMEN': ['Iulius', 'Claudius', 'Flavius', 'Aelius', 'Ulpius', 'Valerius',\n",
        "                      'Aurelius', 'Sempronius', 'Antonius', 'Domitius', 'Cassius', 'Pompeius'],\n",
        "            'COGNOMEN': ['Victor', 'Felix', 'Maximus', 'Severus', 'Priscus', 'Fortunatus',\n",
        "                         'Clemens', 'Primus', 'Secundus', 'Rufus', 'Saturninus', 'Marcellus'],\n",
        "            'TRIBUS': ['Vel', 'Qui', 'Gal', 'Pap', 'Ser', 'Cla', 'Arn', 'Pol', 'Pup', 'Aem', 'Ani','Cam','Clu','Col','Cor','Cru','Esq','Fab','Fal','Hor','Lem','Mae','Men','Ouf','Pal','Pob','Pom','Pup','Qui','Rom','Sab','Sca','Ste','Sub','Ter','Tro','Vel','Vol'],\n",
        "            'ORIGO': ['Roma', 'Hispania', 'Gallia', 'Dacia', 'Syria', 'Berytus', 'Carthago'],\n",
        "            'OCCUPATION': ['miles', 'veteranus', 'medicus', 'scriba', 'negociator', 'faber']\n",
        "        }\n",
        "\n",
        "        for k, v in defaults.items():\n",
        "            if k not in self.pools or not self.pools[k]:\n",
        "                self.pools[k] = v\n",
        "\n",
        "        # EXPANDED FORMULAS (Add Votive Sentences)\n",
        "        self.FORMULAS = [\n",
        "            'D M', 'D M', 'D M',\n",
        "            'D M S', 'Dis Manibus', 'Dis Manibus Sacrum',\n",
        "            'H S E', 'Hic Situs Est',\n",
        "            'I O M', 'Iovi Optimo Maximo',\n",
        "            'V S L M', 'Votum Solvit Libens Merito',\n",
        "            'B M', 'Bene Merenti',\n",
        "            'Ara Larum', 'Deo Herculi', 'Deo Silvano',\n",
        "            'Pro Salute', 'In Honorem', 'Memoriae',\n",
        "            'Posuit', 'Fecit', 'Dedit', 'Dedicavit'\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "        # EXPANDED MILITARY UNITS\n",
        "        self.MILITARY = [\n",
        "            # Legions\n",
        "            'legio II Adiutrix', 'leg. II Adi.', 'leg II', 'legionis II',\n",
        "            'legio X Gemina', 'leg. X Gem.', 'leg X',\n",
        "            'legio XIII Gemina', 'leg. XIII',\n",
        "            'legio VI Victrix', 'leg. VI Vic.',\n",
        "            # Cohorts\n",
        "            'cohors I', 'coh. I', 'coh I',\n",
        "            'cohors II', 'coh. II',\n",
        "            'cohors I Brittonum', 'coh. I Brit.',\n",
        "            # Alae\n",
        "            'ala II Asturum', 'ala II', 'alae II',\n",
        "            # Numeri\n",
        "            'numerus', 'num.', 'vexillatio'\n",
        "        ]\n",
        "\n",
        "        # Ranks triggering force logic\n",
        "        self.RANKS = ['mil.', 'miles', 'veteranus', 'vet.', 'centurio', '7.',\n",
        "                      'tribunus', 'trib. mil.', 'optio', 'praefectus', 'signifer']\n",
        "\n",
        "    def _get(self, category):\n",
        "        return random.choice(self.pools.get(category, ['UNKNOWN']))\n",
        "\n",
        "    def _append(self, parts, spans, text, label=None):\n",
        "        sep = \" \"\n",
        "        # Randomize separators\n",
        "        if random.random() < 0.15 and len(parts) > 0:\n",
        "            sep = \"\\n\"\n",
        "        elif random.random() < 0.05 and len(parts) > 0:\n",
        "            sep = \" · \"\n",
        "\n",
        "        current_text = \"\".join(parts)\n",
        "        if not current_text: sep = \"\"\n",
        "\n",
        "        start_idx = len(current_text) + len(sep)\n",
        "        parts.append(sep + text)\n",
        "\n",
        "        if label:\n",
        "            end_idx = start_idx + len(text)\n",
        "            spans.append([start_idx, end_idx, label])\n",
        "\n",
        "    def generate_funerary(self, force_military=False):\n",
        "        parts = []\n",
        "        spans = []\n",
        "\n",
        "        # 1. Formula\n",
        "        if random.random() < (0.5 if force_military else 0.9):\n",
        "            form = random.choice(self.FORMULAS)\n",
        "            if \" \" in form and form.islower() == False and random.random() < 0.4:\n",
        "                form = \"\".join([w[0].upper() for w in form.split()])\n",
        "            self._append(parts, spans, form, \"FORMULA\")\n",
        "\n",
        "        # 2. Name Generation\n",
        "        case = 'dative' if random.random() < 0.7 else 'nominative'\n",
        "        prae = self._get('PRAENOMEN')\n",
        "        nomen = self._get('NOMEN')\n",
        "        cog = self._get('COGNOMEN')\n",
        "\n",
        "        if case == 'dative':\n",
        "            # FIX 4: Inflect Praenomen too\n",
        "            prae = inflect_praenomen(prae, 'dative')\n",
        "            nomen = inflect_latin(nomen, 'dative')\n",
        "            cog = inflect_latin(cog, 'dative')\n",
        "\n",
        "        self._append(parts, spans, prae, \"PRAENOMEN\")\n",
        "        self._append(parts, spans, nomen, \"NOMEN\")\n",
        "\n",
        "        # 3. Filiation\n",
        "        if random.random() < 0.4:\n",
        "            f_prae = random.choice(['M', 'L', 'C', 'T'])\n",
        "            fil_str = f\"{f_prae}. f.\"\n",
        "            self._append(parts, spans, fil_str, \"RELATIONSHIP\")\n",
        "\n",
        "        # 4. Tribe\n",
        "        if random.random() < 0.5:\n",
        "            self._append(parts, spans, self._get('TRIBUS'), \"TRIBE\")\n",
        "\n",
        "        self._append(parts, spans, cog, \"COGNOMEN\")\n",
        "\n",
        "        # 5. Origo\n",
        "        if random.random() < 0.3:\n",
        "            self._append(parts, spans, self._get('ORIGO'), \"ORIGO\")\n",
        "\n",
        "        # 6. Occupation & Military Unit\n",
        "        if force_military or random.random() < 0.3:\n",
        "\n",
        "            if force_military:\n",
        "                occ = random.choice(self.RANKS)\n",
        "            else:\n",
        "                occ = abbreviate_text(self._get('OCCUPATION'))\n",
        "\n",
        "            self._append(parts, spans, occ, \"OCCUPATION\")\n",
        "\n",
        "            # Logic: Is this a soldier?\n",
        "            is_soldier = force_military or any(x in occ for x in ['mil', 'vet', 'cent', 'trib', 'praef', '7.'])\n",
        "\n",
        "            if is_soldier:\n",
        "                threshold = 1.0 if force_military else 0.8\n",
        "                if random.random() < threshold:\n",
        "                    unit = random.choice(self.MILITARY)\n",
        "                    if '.' in unit and random.random() < 0.4:\n",
        "                        unit = unit.replace('.', '')\n",
        "                    self._append(parts, spans, unit, \"MILITARY_UNIT\")\n",
        "\n",
        "        # 7. Age\n",
        "        if random.random() < 0.8:\n",
        "            years = random.randint(1, 90)\n",
        "            # FEATURE: Convert to Roman Numerals (95% of time)\n",
        "            if random.random() < 0.95:\n",
        "                years_str = to_roman(years)\n",
        "            else:\n",
        "                years_str = str(years)\n",
        "\n",
        "            variations = [\n",
        "                f\"vixit annis {years_str}\",\n",
        "                f\"v. an. {years_str}\",\n",
        "                f\"annorum {years_str}\",\n",
        "                f\"ann. {years_str}\",\n",
        "                f\"qui vixit ann. {years_str}\"\n",
        "            ]\n",
        "\n",
        "            if random.random() < 0.4:\n",
        "                age_str = f\"annorum {years_str}\"\n",
        "            else:\n",
        "                age_str = random.choice(variations)\n",
        "            self._append(parts, spans, age_str, \"AGE\")\n",
        "\n",
        "         # 8. Dedicator\n",
        "        if random.random() < 0.6:\n",
        "            ded_nomen = self._get('NOMEN')\n",
        "            ded_cog = self._get('COGNOMEN')\n",
        "            rel = abbreviate_text(random.choice(['coniunx', 'frater', 'heres', 'pater', 'filius']))\n",
        "\n",
        "            self._append(parts, spans, ded_nomen, \"NOMEN\")\n",
        "            self._append(parts, spans, ded_cog, \"COGNOMEN\")\n",
        "            self._append(parts, spans, rel, \"RELATIONSHIP\")\n",
        "\n",
        "            verb = random.choice(['fecit', 'posuit', 'faciendum curavit'])\n",
        "            self._append(parts, spans, abbreviate_text(verb), \"VERB\")\n",
        "\n",
        "        full_text = \"\".join(parts)\n",
        "        return {\"id\": f\"syn_{random.randint(100,999999)}\", \"text\": full_text, \"annotations\": spans}\n",
        "\n",
        "    def generate_peregrine(self):\n",
        "        \"\"\"\n",
        "        Generates inscriptions for non-citizens (Peregrines).\n",
        "        Structure: Cognomen + Filiation (Father's Name) + Age.\n",
        "        Example: \"Severus Tongini f. annorum XXI\"\n",
        "        \"\"\"\n",
        "        parts = []\n",
        "        spans = []\n",
        "\n",
        "        # 1. Formula (D M is common even for peregrines later on)\n",
        "        if random.random() < 0.8:\n",
        "            form = random.choice(self.FORMULAS)\n",
        "            self._append(parts, spans, form, \"FORMULA\")\n",
        "\n",
        "        # 2. Name: Single Name (Cognomen)\n",
        "        # We pick from COGNOMEN pool but treat it as the primary identifier\n",
        "        name = self._get('COGNOMEN')\n",
        "        self._append(parts, spans, name, \"COGNOMEN\")\n",
        "\n",
        "        # 3. Filiation: \"Tongini f.\" (Father's name in Genitive + f.)\n",
        "        if random.random() < 0.9: # High chance for peregrines\n",
        "            father = self._get('COGNOMEN')\n",
        "            # Heuristic: Convert Nominative to Genitive-ish\n",
        "            # Severus -> Severi, Tonginus -> Tongini\n",
        "            if father.endswith('us'): father_gen = father[:-2] + 'i'\n",
        "            elif father.endswith('ius'): father_gen = father[:-3] + 'i'\n",
        "            elif father.endswith('a'): father_gen = father + 'e'\n",
        "            else: father_gen = father # Fallback\n",
        "\n",
        "            rel_str = f\"{father_gen} f.\"\n",
        "            self._append(parts, spans, rel_str, \"RELATIONSHIP\")\n",
        "\n",
        "        # 4. Age (CRITICAL: Use 'annorum' here to fix your Real Data gap)\n",
        "        if random.random() < 0.9:\n",
        "            years = random.randint(5, 80)\n",
        "            years_str = to_roman(years)\n",
        "\n",
        "            # Mix of 'vixit annis' and 'annorum' (Genitive Plural)\n",
        "            variations = [\n",
        "                f\"annorum {years_str}\",  # Matches 'Annorum XXI' in real data\n",
        "                f\"ann. {years_str}\",\n",
        "                f\"an. {years_str}\",\n",
        "                f\"vixit annis {years_str}\"\n",
        "            ]\n",
        "            age_str = random.choice(variations)\n",
        "            self._append(parts, spans, age_str, \"AGE\")\n",
        "\n",
        "        # 5. Formula (Footer)\n",
        "        if random.random() < 0.6:\n",
        "            # H S E (Hic Situs Est) is very common for this demographic\n",
        "            form = \"H S E\" if random.random() < 0.5 else \"Hic Situs Est\"\n",
        "            if random.random() < 0.5:\n",
        "                form += \" S T T L\" # Sit Tibi Terra Levis\n",
        "            self._append(parts, spans, form, \"FORMULA\")\n",
        "\n",
        "        full_text = \"\".join(parts)\n",
        "        return {\"id\": f\"syn_{random.randint(100,999999)}\", \"text\": full_text, \"annotations\": spans}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load Stats\n",
        "    try:\n",
        "        print(\"Extracting stats from real data (assets/output.jsonl)...\")\n",
        "        # Make sure this file path points to your real data\n",
        "        counts = FrequencyExtractor.extract_frequencies('assets/output.jsonl')\n",
        "        pools = {k: FrequencyExtractor.get_weighted_list(v) for k, v in counts.items()}\n",
        "        print(\"Stats loaded and filtered.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load real data ({e}). Using built-in defaults.\")\n",
        "        pools = None\n",
        "\n",
        "    gen = InscriptionGenerator(pools)\n",
        "\n",
        "    output_file = 'assets/synthetic_data.jsonl'\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "\n",
        "        # Batch 1: General Population (Match Real Data Size ~1400)\n",
        "        print(\"Generating  General Inscriptions...\")\n",
        "        for _ in range(600):\n",
        "            entry = gen.generate_funerary(force_military=False)\n",
        "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        # Batch 2: Military Reinforcements (Boost Weak Class ~600)\n",
        "        print(\"Generating  Military-Focused Inscriptions...\")\n",
        "        for _ in range(600):\n",
        "            entry = gen.generate_funerary(force_military=True)\n",
        "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        # 3. Peregrine/Single Names (Fixes \"Severus Tongini\") - 600 records\n",
        "        print(\"Generating  Peregrine Inscriptions...\")\n",
        "        for _ in range(1800):\n",
        "            entry = gen.generate_peregrine() # <--- Calling the new method\n",
        "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"✅ Done. Saved records to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjPUNOzHLpWf",
        "outputId": "22744043-9321-4e73-8dcb-94d3d17ea435"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting stats from real data (assets/output.jsonl)...\n",
            "Stats loaded and filtered.\n",
            "Generating  General Inscriptions...\n",
            "Generating  Military-Focused Inscriptions...\n",
            "Generating  Peregrine Inscriptions...\n",
            "✅ Done. Saved records to assets/synthetic_data.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Check sample output\n",
        "with open('assets/synthetic_data.jsonl') as f:\n",
        "    samples = [json.loads(f.readline()) for _ in range(10)]\n",
        "    print(\"\\nSample synthetic inscriptions:\")\n",
        "    for sample in samples:\n",
        "        print(f\"\\n  {sample['text']}\")\n",
        "        labels = [ann[2] for ann in sample['annotations']]\n",
        "        print(f\"  Labels: {set(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEK1LZWuuu6e",
        "outputId": "3be8ac8b-589a-4e7e-c921-c2e39238ddeb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample synthetic inscriptions:\n",
            "\n",
            "  AL Caio Mario\n",
            "Galeria Mestrio annorum LXI Cornelius Optatus f.\n",
            "faciendum curavit\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'TRIBE', 'VERB', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  Dedicavit Lucio Valerio M. f. Pub Macro annorum XXVI Iulius Niger pater fac.\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'TRIBE', 'VERB', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  In Honorem Marco Fabio Felix annorum LXXV Baebius Capito fr. p.\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'VERB', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  H S E Lucio Aemilio Optato vixit annis XXXIX\n",
            "  Labels: {'PRAENOMEN', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  IOM Caius\n",
            "Iulius · L. f. Verus mil.\n",
            "Octavius Cassianus f.\n",
            "fec.\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'VERB', 'OCCUPATION', 'COGNOMEN', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  Marco Valerio\n",
            "Avito\n",
            "  Labels: {'NOMEN', 'COGNOMEN', 'PRAENOMEN'}\n",
            "\n",
            "  HSE Quintus Attius C. f. Ani Maximus annorum LXXXI\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'TRIBE', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  Iovi Optimo Maximo Caio Attio M. f. Maximo Valerius Sabinus con. · p.\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'VERB', 'COGNOMEN', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  Dis Manibus Lucius Valerius\n",
            "T. f. Privatus\n",
            "Annius Sabinus · filius f.\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'VERB', 'COGNOMEN', 'NOMEN', 'FORMULA'}\n",
            "\n",
            "  In Honorem Caio Aurelio L. f.\n",
            "Iusto Patriciensis vixit annis XXXVIII\n",
            "  Labels: {'PRAENOMEN', 'RELATIONSHIP', 'ORIGO', 'COGNOMEN', 'AGE', 'NOMEN', 'FORMULA'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## combine real & synthethic"
      ],
      "metadata": {
        "id": "QrrC2vGtS63j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #!/usr/bin/env python3\n",
        "#\n",
        "\"\"\"\n",
        "Harmonize real and synthetic data, resolve overlaps, and create stratified splits.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Centralized Mapping\n",
        "# Ensure that keys cover both your Real Data CSV columns AND Synthetic Generator outputs\n",
        "LABEL_MAPPING = {\n",
        "    # --- INPUT (Left) : OUTPUT (Right) ---\n",
        "\n",
        "    # Real Data uses 'PRAENOMEN', Syn uses 'PRAENOMEN' -> Target: PRAENOMEN\n",
        "    \"PRAENOMEN\": \"PRAENOMEN\",\n",
        "    \"NOMEN\": \"NOMEN\",\n",
        "    \"COGNOMEN\": \"COGNOMEN\",\n",
        "\n",
        "    # Real Data uses 'TRIBUS', Syn uses 'TRIBE' -> Target: TRIBE (or TRIBUS, pick one)\n",
        "    \"TRIBUS\": \"TRIBE\",\n",
        "    \"TRIBE\": \"TRIBE\",\n",
        "\n",
        "    # Real Data uses 'ORIGO', Syn uses 'ORIGO' -> Target: ORIGO\n",
        "    # (We map 'ORIGIN' just in case an old synthetic batch has it)\n",
        "    \"ORIGO\": \"ORIGO\",\n",
        "    \"ORIGIN\": \"ORIGO\",\n",
        "\n",
        "    # Occupations\n",
        "    \"OCCUPATION\": \"OCCUPATION\",\n",
        "    \"MILITARY_UNIT\": \"MILITARY_UNIT\",\n",
        "\n",
        "    # Formulas\n",
        "    \"FUNERARY_FORMULA\": \"FORMULA\",\n",
        "    \"DEDICATORY_FORMULA\": \"FORMULA\",\n",
        "    \"DEDICATION_TO_THE_GODS\": \"FORMULA\",\n",
        "    \"FORMULA\": \"FORMULA\",\n",
        "\n",
        "    # The Fix for the 0.00 Score (Merge BENE_MERENTI into FORMULA)\n",
        "    \"BENE_MERENTI\": \"FORMULA\",\n",
        "\n",
        "    # Relations\n",
        "    \"RELATIONSHIP\": \"RELATIONSHIP\",\n",
        "    \"FILIATION\": \"RELATIONSHIP\",\n",
        "\n",
        "    # Age\n",
        "    \"AGE\": \"AGE\",\n",
        "    \"AGE_PHRASE\": \"AGE\",\n",
        "\n",
        "    # DROPPED LABELS (Noise reduction)\n",
        "    \"EPITHET\": None,           # Too generic / confused with Cognomen\n",
        "    \"AGE_YEARS\": None,\n",
        "    \"AGE_DAYS\": None,\n",
        "    \"ACTION\": None,\n",
        "    \"VERB\": None,\n",
        "    \"SUPERNOMEN\": \"COGNOMEN\"   # Treat supernomen as just another name\n",
        "}\n",
        "\n",
        "def resolve_overlaps(annotations):\n",
        "    \"\"\"\n",
        "    Greedy overlap resolution:\n",
        "    1. Sort by length (descending) -> Keep longest spans first.\n",
        "    2. If lengths equal, sort by start position.\n",
        "    \"\"\"\n",
        "    if not annotations:\n",
        "        return []\n",
        "\n",
        "    # Sort: Longest spans first, then by start position\n",
        "    # x[1] - x[0] is length\n",
        "    sorted_anns = sorted(annotations, key=lambda x: (x[1] - x[0], x[0]), reverse=True)\n",
        "\n",
        "    kept = []\n",
        "    # Create a set of occupied indices\n",
        "    occupied = set()\n",
        "\n",
        "    for start, end, label in sorted_anns:\n",
        "        # Check if any character in this span is already occupied\n",
        "        span_indices = set(range(start, end))\n",
        "        if not span_indices & occupied:\n",
        "            # No overlap, keep it\n",
        "            kept.append([start, end, label])\n",
        "            occupied.update(span_indices)\n",
        "\n",
        "    # Sort back by start position for clean JSONL\n",
        "    return sorted(kept, key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def harmonize_file(input_path, label_map):\n",
        "    \"\"\"\n",
        "    Reads a JSONL, maps labels, removes overlaps, returns list of dicts.\n",
        "    \"\"\"\n",
        "    cleaned_records = []\n",
        "    dropped_anns = 0\n",
        "    total_anns = 0\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '') or record.get('transcription', '')\n",
        "            # Ensure text isn't empty\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Normalize annotations\n",
        "            raw_anns = record.get('annotations', [])\n",
        "            mapped_anns = []\n",
        "\n",
        "            for entity in raw_anns:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                s, e, original_label = entity\n",
        "\n",
        "                # Check Bounds\n",
        "                if s < 0 or e > len(text) or s >= e:\n",
        "                    continue\n",
        "\n",
        "                # Map Label\n",
        "                # Use upper case to ensure case-insensitive matching\n",
        "                target_label = label_map.get(original_label.upper())\n",
        "\n",
        "                total_anns += 1\n",
        "                if target_label:\n",
        "                    mapped_anns.append([s, e, target_label])\n",
        "                else:\n",
        "                    dropped_anns += 1\n",
        "\n",
        "            # Resolve Overlaps\n",
        "            final_anns = resolve_overlaps(mapped_anns)\n",
        "\n",
        "            # Construct cleaned record\n",
        "            if final_anns:\n",
        "                cleaned_records.append({\n",
        "                    \"text\": text,\n",
        "                    \"annotations\": final_anns,\n",
        "                    \"meta\": {\"source\": input_path} # Useful for debugging\n",
        "                })\n",
        "\n",
        "    print(f\"Processed {input_path}:\")\n",
        "    print(f\"  - Kept Records: {len(cleaned_records)}\")\n",
        "    print(f\"  - Dropped Annotations: {dropped_anns}/{total_anns}\")\n",
        "\n",
        "    return cleaned_records\n",
        "\n",
        "def combine_and_split(real_path, synthetic_path, train_out, dev_out):\n",
        "    print(\"=== HARMONIZING DATA ===\")\n",
        "\n",
        "    # 1. Process BOTH files through the same logic\n",
        "    real_data = harmonize_file(real_path, LABEL_MAPPING)\n",
        "    syn_data = harmonize_file(synthetic_path, LABEL_MAPPING)\n",
        "\n",
        "    # 2. Add 'source_type' tag for stratification\n",
        "    for r in real_data: r['stratify_tag'] = 'real'\n",
        "    for r in syn_data: r['stratify_tag'] = 'synthetic'\n",
        "\n",
        "    combined = real_data + syn_data\n",
        "    stratify_labels = [r['stratify_tag'] for r in combined]\n",
        "\n",
        "    print(f\"\\nTotal Combined: {len(combined)}\")\n",
        "\n",
        "    # 3. Stratified Split\n",
        "    # This ensures your Dev set has exactly 20% Real and 20% Synthetic\n",
        "    train, dev = train_test_split(\n",
        "        combined,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=stratify_labels\n",
        "    )\n",
        "\n",
        "    # 4. Write Output (Removing temporary meta tags)\n",
        "    def write_jsonl(data, filename):\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for record in data:\n",
        "                # specific clean up before save\n",
        "                out_record = {\n",
        "                    \"text\": record['text'],\n",
        "                    \"annotations\": record['annotations']\n",
        "                }\n",
        "                f.write(json.dumps(out_record, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    write_jsonl(train, train_out)\n",
        "    write_jsonl(dev, dev_out)\n",
        "\n",
        "    print(f\"\\n✅ Split Complete (Stratified):\")\n",
        "    print(f\"   Train: {len(train)}\")\n",
        "    print(f\"   Dev:   {len(dev)}\")\n",
        "    print(f\"   Files saved to {train_out} and {dev_out}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update paths to match your actual filenames\n",
        "    combine_and_split(\n",
        "        real_path='assets/real_inscriptions_clean.jsonl',  # From the first script\n",
        "        synthetic_path='assets/synthetic_data.jsonl', # From the generator\n",
        "        train_out='assets/train.jsonl',\n",
        "        dev_out='assets/dev.jsonl'\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiWNnvEYSHwr",
        "outputId": "0dfb45c3-a758-4f59-d082-1a0ebc39baff"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HARMONIZING DATA ===\n",
            "Processed assets/real_inscriptions_clean.jsonl:\n",
            "  - Kept Records: 1393\n",
            "  - Dropped Annotations: 2292/11246\n",
            "Processed assets/synthetic_data.jsonl:\n",
            "  - Kept Records: 3000\n",
            "  - Dropped Annotations: 705/18573\n",
            "\n",
            "Total Combined: 4393\n",
            "\n",
            "✅ Split Complete (Stratified):\n",
            "   Train: 3514\n",
            "   Dev:   879\n",
            "   Files saved to assets/train.jsonl and assets/dev.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "STEP 2: Clean text (remove Leiden brackets) and re-align annotations.\n",
        "Fixes multi-word Praenomina.\n",
        "\"\"\"\n",
        "import json\n",
        "\n",
        "def clean_text_and_spans(input_path, output_path):\n",
        "    cleaned_records = []\n",
        "    fixed_praenomina = 0\n",
        "\n",
        "    print(f\"Repairing {input_path}...\")\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "            except: continue\n",
        "\n",
        "            original_text = record.get('text', '')\n",
        "            original_anns = record.get('annotations', [])\n",
        "\n",
        "            if not original_text: continue\n",
        "\n",
        "            # --- STEP 1: CLEAN TEXT & BUILD MAP ---\n",
        "            # Rebuild text char by char. mapping[old_index] = new_index\n",
        "            new_text = \"\"\n",
        "            mapping = []\n",
        "\n",
        "            for char in original_text:\n",
        "                mapping.append(len(new_text))\n",
        "\n",
        "                # REMOVE CHARACTERS\n",
        "                if char in ['/', '{', '}', '(', ')', '[', ']']:\n",
        "                    # Replace '/' with space to prevent word merging\n",
        "                    if char == '/':\n",
        "                        new_text += \" \"\n",
        "                    # Drop brackets entirely\n",
        "                    else:\n",
        "                        pass\n",
        "                else:\n",
        "                    new_text += char\n",
        "\n",
        "            mapping.append(len(new_text)) # Map the end of string\n",
        "\n",
        "            # --- STEP 2: REMAP ANNOTATIONS ---\n",
        "            new_anns = []\n",
        "            for start, end, label in original_anns:\n",
        "                if start >= len(mapping) or end >= len(mapping):\n",
        "                    continue\n",
        "\n",
        "                new_start = mapping[start]\n",
        "                new_end = mapping[end]\n",
        "\n",
        "                if new_start == new_end: continue # Annotation collapsed\n",
        "\n",
        "                span_text = new_text[new_start:new_end]\n",
        "\n",
        "                # --- STEP 3: FIX BAD PRAENOMINA ---\n",
        "                # Real data often labels \"Titus Flavius\" as one PRAENOMEN.\n",
        "                # We fix this by shrinking the span to the first word only.\n",
        "                if label == \"PRAENOMEN\" and \" \" in span_text.strip():\n",
        "                    space_index = span_text.strip().find(\" \")\n",
        "                    if space_index > 0:\n",
        "                        # Adjust end to the first space\n",
        "                        # Note: We do this relative to the span text\n",
        "                        actual_space_index = span_text.find(\" \")\n",
        "                        new_end = new_start + actual_space_index\n",
        "                        fixed_praenomina += 1\n",
        "\n",
        "                # Clean leading/trailing whitespace in span\n",
        "                while new_end > new_start and new_text[new_end-1] == \" \":\n",
        "                    new_end -= 1\n",
        "                while new_start < new_end and new_text[new_start] == \" \":\n",
        "                    new_start += 1\n",
        "\n",
        "                if new_end > new_start:\n",
        "                    new_anns.append([new_start, new_end, label])\n",
        "\n",
        "            record['text'] = new_text\n",
        "            record['annotations'] = new_anns\n",
        "            cleaned_records.append(record)\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for r in cleaned_records:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"  - Fixed {fixed_praenomina} bad Praenomina.\")\n",
        "    print(f\"  - Saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_text_and_spans('assets/train.jsonl', 'assets/train_clean.jsonl')\n",
        "    clean_text_and_spans('assets/dev.jsonl', 'assets/dev_clean.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J15qx9NYUO7I",
        "outputId": "0efc6504-128f-47bb-8291-7ef3b36afc3a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repairing assets/train.jsonl...\n",
            "  - Fixed 23 bad Praenomina.\n",
            "  - Saved to assets/train_clean.jsonl\n",
            "Repairing assets/dev.jsonl...\n",
            "  - Fixed 7 bad Praenomina.\n",
            "  - Saved to assets/dev_clean.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "STEP 3: Align character indices to tokens (JSONL -> JSONL).\n",
        "\"\"\"\n",
        "import spacy\n",
        "import json\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "def align_annotations(input_path, output_path):\n",
        "    # Load model (ensure this matches what you will train with later)\n",
        "    try:\n",
        "        nlp = spacy.load('la_core_web_lg')\n",
        "    except OSError:\n",
        "        print(\"Warning: 'la_core_web_lg' not found. Using blank 'la'.\")\n",
        "        nlp = spacy.blank(\"la\")\n",
        "\n",
        "    corrected_records = []\n",
        "    stats = {\n",
        "        \"total\": 0,\n",
        "        \"perfect\": 0,\n",
        "        \"fixed\": 0,\n",
        "        \"malformed\": 0,\n",
        "        \"dropped_align\": 0,\n",
        "        \"dropped_overlap\": 0\n",
        "    }\n",
        "\n",
        "    print(f\"Aligning {input_path}...\")\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            doc = nlp.make_doc(text)\n",
        "            candidate_spans = []\n",
        "\n",
        "            # 1. First Pass: Create Span objects\n",
        "            for entity in record.get('annotations', []):\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "\n",
        "                # Sanity check indices\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                # Align\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is None:\n",
        "                    stats[\"dropped_align\"] += 1\n",
        "                else:\n",
        "                    # Check if alignment changed the indices\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        stats[\"perfect\"] += 1\n",
        "                    else:\n",
        "                        stats[\"fixed\"] += 1\n",
        "\n",
        "                    candidate_spans.append(span)\n",
        "\n",
        "            # 2. Second Pass: Filter Overlaps (CRITICAL)\n",
        "            # alignment_mode=\"expand\" can introduce overlaps.\n",
        "            # filter_spans keeps the longest span and discards conflicts.\n",
        "            original_count = len(candidate_spans)\n",
        "            final_spans = filter_spans(candidate_spans)\n",
        "\n",
        "            if len(final_spans) < original_count:\n",
        "                stats[\"dropped_overlap\"] += (original_count - len(final_spans))\n",
        "\n",
        "            # 3. Serialize back to JSON format\n",
        "            clean_annotations = []\n",
        "            for span in final_spans:\n",
        "                clean_annotations.append([span.start_char, span.end_char, span.label_])\n",
        "\n",
        "            record['annotations'] = clean_annotations\n",
        "            corrected_records.append(record)\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for record in corrected_records:\n",
        "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"✅ Aligned {input_path} -> {output_path}\")\n",
        "    print(f\"   Perfect Match:   {stats['perfect']}\")\n",
        "    print(f\"   Realigned:       {stats['fixed']}\")\n",
        "    print(f\"   Dropped (Align): {stats['dropped_align']} (No matching token boundary)\")\n",
        "    print(f\"   Dropped (Over):  {stats['dropped_overlap']} (Created conflict)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    align_annotations('assets/train_clean.jsonl', 'assets/train_focused_aligned.jsonl')\n",
        "    align_annotations('assets/dev_clean.jsonl', 'assets/dev_focused_aligned.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEuAI4amusm5",
        "outputId": "926f754c-0a13-441e-a7d2-61cba919c634"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligning assets/train_clean.jsonl...\n",
            "✅ Aligned assets/train_clean.jsonl -> assets/train_focused_aligned.jsonl\n",
            "   Perfect Match:   18414\n",
            "   Realigned:       1462\n",
            "   Dropped (Align): 42 (No matching token boundary)\n",
            "   Dropped (Over):  303 (Created conflict)\n",
            "Aligning assets/dev_clean.jsonl...\n",
            "✅ Aligned assets/dev_clean.jsonl -> assets/dev_focused_aligned.jsonl\n",
            "   Perfect Match:   4767\n",
            "   Realigned:       345\n",
            "   Dropped (Align): 3 (No matching token boundary)\n",
            "   Dropped (Over):  78 (Created conflict)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Convert to .spacy format\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "def create_spacy_file(input_path, output_path, model='la_core_web_lg'):\n",
        "    nlp = spacy.load(model)\n",
        "    db = DocBin()\n",
        "\n",
        "    with open(input_path) as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "\n",
        "            for start, end, label in record.get('annotations', []):\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "                if span:\n",
        "                    ents.append(span)\n",
        "\n",
        "            doc.ents = filter_spans(ents)\n",
        "            db.add(doc)\n",
        "\n",
        "    db.to_disk(output_path)\n",
        "    print(f\"✅ Created {output_path}\")\n",
        "\n",
        "create_spacy_file('assets/train_focused_aligned.jsonl', 'corpus/train.spacy')\n",
        "create_spacy_file('assets/dev_focused_aligned.jsonl', 'corpus/dev.spacy')\n",
        "\n",
        "print(\"\\n✅ Ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjNtwP3byyUT",
        "outputId": "8917b8ce-c463-4727-f9c3-d508a23a3649"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created corpus/train.spacy\n",
            "✅ Created corpus/dev.spacy\n",
            "\n",
            "✅ Ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the thing"
      ],
      "metadata": {
        "id": "acOqVgR8jgrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Generate the base config ---\n",
        "!python -m spacy init config configs/config.cfg --lang la --pipeline tok2vec,ner --optimize accuracy --force\n",
        "\n",
        "print(\"✅ Base 'config.cfg' generated.\")\n",
        "\n",
        "# --- 2. Load and Modify ---\n",
        "config_path = Path(\"configs/config.cfg\")\n",
        "config = spacy.util.load_config(config_path)\n",
        "\n",
        "# Define the model we are using\n",
        "LATIN_MODEL = \"la_core_web_lg\"\n",
        "\n",
        "# --- Part A: Initialize Vectors (CRITICAL FOR LG MODELS) ---\n",
        "# This loads the 300-dim vectors into the vocab so the tok2vec layer can find them.\n",
        "config[\"initialize\"][\"vectors\"] = LATIN_MODEL\n",
        "\n",
        "# --- Part B: Source the tok2vec component ---\n",
        "config[\"components\"][\"tok2vec\"] = {\n",
        "    \"source\": LATIN_MODEL,\n",
        "    \"component\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "# --- Part C: Connect NER to the vectors ---\n",
        "# The tok2vec OUTPUT width is 96, even if the input vectors are 300.\n",
        "config[\"components\"][\"ner\"][\"model\"][\"tok2vec\"] = {\n",
        "    \"@architectures\": \"spacy.Tok2VecListener.v1\",\n",
        "    \"width\": 96,\n",
        "    \"upstream\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "config[\"nlp\"][\"batch_size\"] = 200\n",
        "\n",
        "# --- Part D: Paths and Freezing ---\n",
        "config[\"paths\"][\"train\"] = \"./corpus/train.spacy\"\n",
        "config[\"paths\"][\"dev\"] = \"./corpus/dev.spacy\"\n",
        "\n",
        "# Freeze tok2vec so we don't ruin the pretrained Latin intelligence\n",
        "config[\"training\"][\"frozen_components\"] = [\"tok2vec\"]\n",
        "# or unfreeze it, see what happens\n",
        "#config[\"training\"][\"frozen_components\"] = []\n",
        "#config[\"training\"][\"max_epochs\"]= 100\n",
        "config[\"training\"][\"max_epochs\"] = 30\n",
        "# Mark it as annotating so it actually runs\n",
        "config[\"training\"][\"annotating_components\"] = [\"tok2vec\"]\n",
        "\n",
        "# --- 3. Save ---\n",
        "config.to_disk(config_path)\n",
        "\n",
        "print(f\"✅ Config updated for {LATIN_MODEL}. Listener width set to 96 (correct output dim).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mKD51UtI_uY",
        "outputId": "fd781642-6ca0-4a1c-fd6b-aceb560c8b6e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: la\n",
            "- Pipeline: ner\n",
            "- Optimize for: accuracy\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "configs/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "✅ Base 'config.cfg' generated.\n",
            "✅ Config updated for la_core_web_lg. Listener width set to 96 (correct output dim).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process!\n",
        "!python -m spacy train configs/config.cfg --output ./training/ #--gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXBY8quJEL5",
        "outputId": "4be858fb-2fcb-49cf-c5b4-793d9e4a9f5c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: training\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Frozen components: ['tok2vec']\u001b[0m\n",
            "\u001b[38;5;4mℹ Set annotations on update for: ['tok2vec']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  --------  ------  ------  ------  ------\n",
            "  0       0     68.19   11.26    7.77   20.44    0.11\n",
            "  0     200   9154.33   64.95   71.81   59.30    0.65\n",
            "  0     400   7610.89   74.07   78.44   70.16    0.74\n",
            "  1     600   7532.65   75.43   78.03   73.00    0.75\n",
            "  2     800   9725.54   77.11   82.42   72.45    0.77\n",
            "  3    1000  10376.21   78.37   81.33   75.63    0.78\n",
            "  4    1200  11351.72   79.56   86.24   73.84    0.80\n",
            "  5    1400  15119.68   79.79   84.12   75.88    0.80\n",
            "  7    1600  17112.00   80.01   83.63   76.70    0.80\n",
            "  9    1800  19837.69   80.30   88.04   73.82    0.80\n",
            " 11    2000  24494.75   80.72   86.93   75.33    0.81\n",
            " 14    2200  27854.93   80.62   86.62   75.39    0.81\n",
            " 18    2400  34314.41   80.88   85.98   76.36    0.81\n",
            " 21    2600  34529.45   80.97   85.32   77.04    0.81\n",
            " 25    2800  34370.49   81.33   86.92   76.42    0.81\n",
            " 29    3000  33346.26   81.22   86.72   76.38    0.81\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Per-label evaluation\n",
        "from spacy.training import Example\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "\n",
        "# Load dev.spacy\n",
        "dev_docs = list(DocBin().from_disk(\"corpus/dev.spacy\").get_docs(nlp.vocab))\n",
        "\n",
        "# Score by label\n",
        "from collections import defaultdict\n",
        "scores = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
        "\n",
        "for doc in dev_docs:\n",
        "    gold_ents = {(e.start_char, e.end_char, e.label_) for e in doc.ents}\n",
        "    pred_ents = {(e.start_char, e.end_char, e.label_) for e in nlp(doc.text).ents}\n",
        "\n",
        "    for label in set([e[2] for e in gold_ents | pred_ents]):\n",
        "        gold_with_label = {e for e in gold_ents if e[2] == label}\n",
        "        pred_with_label = {e for e in pred_ents if e[2] == label}\n",
        "\n",
        "        scores[label][\"tp\"] += len(gold_with_label & pred_with_label)\n",
        "        scores[label][\"fp\"] += len(pred_with_label - gold_with_label)\n",
        "        scores[label][\"fn\"] += len(gold_with_label - pred_with_label)\n",
        "\n",
        "print(\"\\nPer-label F1 on DEV set:\")\n",
        "print(f\"{'LABEL':<25} {'PREC':<8} {'REC':<8} {'F1':<8}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for label in sorted(scores.keys()):\n",
        "    tp = scores[label][\"tp\"]\n",
        "    fp = scores[label][\"fp\"]\n",
        "    fn = scores[label][\"fn\"]\n",
        "\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
        "\n",
        "    print(f\"{label:<25} {prec:.2f}     {rec:.2f}     {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gunwmI3-GJQM",
        "outputId": "b633a7de-c231-4ecd-b1f4-0dbe495cc653"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-label F1 on DEV set:\n",
            "LABEL                     PREC     REC      F1      \n",
            "--------------------------------------------------\n",
            "AGE                       0.96     0.96     0.96\n",
            "COGNOMEN                  0.88     0.78     0.83\n",
            "FORMULA                   0.94     0.68     0.79\n",
            "MILITARY_UNIT             0.88     0.73     0.80\n",
            "NOMEN                     0.75     0.76     0.75\n",
            "OCCUPATION                0.84     0.63     0.72\n",
            "ORIGO                     0.73     0.46     0.57\n",
            "PRAENOMEN                 0.78     0.84     0.80\n",
            "RELATIONSHIP              0.97     0.79     0.87\n",
            "TRIBE                     0.75     0.73     0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## checking against the real/synth held-back data\n",
        "# Load the trained model and dev data\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "dev_docs = list(DocBin().from_disk(\"corpus/dev.spacy\").get_docs(nlp.vocab))\n",
        "\n",
        "print(f\"📊 Loaded {len(dev_docs)} dev inscriptions\\n\")\n",
        "\n",
        "# Quick visual comparison: predictions vs gold labels\n",
        "print(\"=\" * 100)\n",
        "print(\"SAMPLE PREDICTIONS vs GOLD LABELS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for i, gold_doc in enumerate(dev_docs[:10], 1):  # First 10 samples\n",
        "    text = gold_doc.text\n",
        "    pred_doc = nlp(text)\n",
        "\n",
        "    print(f\"\\n[{i}] {text}\")\n",
        "    print(f\"    {'ENTITY':<30} {'PREDICTED':<25} {'GOLD':<25} {'MATCH'}\")\n",
        "    print(f\"    {'-'*30} {'-'*25} {'-'*25} {'-'*5}\")\n",
        "\n",
        "    # Collect all entity positions\n",
        "    all_positions = set()\n",
        "    pred_ents = {(e.start_char, e.end_char): (e.text, e.label_) for e in pred_doc.ents}\n",
        "    gold_ents = {(e.start_char, e.end_char): (e.text, e.label_) for e in gold_doc.ents}\n",
        "    all_positions.update(pred_ents.keys())\n",
        "    all_positions.update(gold_ents.keys())\n",
        "\n",
        "    # Compare\n",
        "    for pos in sorted(all_positions):\n",
        "        pred = pred_ents.get(pos, ('-', '-'))\n",
        "        gold = gold_ents.get(pos, ('-', '-'))\n",
        "\n",
        "        match = \"✅\" if pred == gold else \"❌\"\n",
        "        entity_text = pred[0] if pred[0] != '-' else gold[0]\n",
        "\n",
        "        print(f\"    {entity_text:<30} {pred[1]:<25} {gold[1]:<25} {match}\")\n",
        "\n",
        "# Quick accuracy metrics\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"ACCURACY METRICS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "total_pred = sum(len(nlp(doc.text).ents) for doc in dev_docs)\n",
        "total_gold = sum(len(doc.ents) for doc in dev_docs)\n",
        "correct = 0\n",
        "\n",
        "for gold_doc in dev_docs:\n",
        "    pred_doc = nlp(gold_doc.text)\n",
        "    pred_set = {(e.start_char, e.end_char, e.label_) for e in pred_doc.ents}\n",
        "    gold_set = {(e.start_char, e.end_char, e.label_) for e in gold_doc.ents}\n",
        "    correct += len(pred_set & gold_set)\n",
        "\n",
        "precision = correct / total_pred if total_pred > 0 else 0\n",
        "recall = correct / total_gold if total_gold > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\nTotal Predictions: {total_pred}\")\n",
        "print(f\"Total Gold Labels: {total_gold}\")\n",
        "print(f\"Correct (Exact Match): {correct}\")\n",
        "print(f\"\\n📈 Precision: {precision:.3f}\")\n",
        "print(f\"📈 Recall:    {recall:.3f}\")\n",
        "print(f\"📈 F1 Score:  {f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz6pnwS2Qllt",
        "outputId": "31141c22-c0b3-43fa-ccb6-2702b69d9875"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Loaded 879 dev inscriptions\n",
            "\n",
            "====================================================================================================\n",
            "SAMPLE PREDICTIONS vs GOLD LABELS\n",
            "====================================================================================================\n",
            "\n",
            "[1] Caius Lucretius Cai filius Papiria Iustus Opitergio miles legionis XV Apollinaris stipendiorum X annorum XXX heres de propio\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Caius                          PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Lucretius                      NOMEN                     NOMEN                     ✅\n",
            "    Papiria Iustus                 TRIBE                     TRIBE                     ✅\n",
            "\n",
            "[2] D M S · Quintus Iulius Albanus faber · annorum XLVI\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    D M S                          FORMULA                   FORMULA                   ✅\n",
            "    Quintus                        PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Iulius                         NOMEN                     NOMEN                     ✅\n",
            "    Albanus                        COGNOMEN                  COGNOMEN                  ✅\n",
            "    faber                          OCCUPATION                OCCUPATION                ✅\n",
            "    annorum XLVI                   AGE                       AGE                       ✅\n",
            "\n",
            "[3] Dis Manibus Saturninus Laeti f. ann. XXV\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Dis Manibus                    FORMULA                   FORMULA                   ✅\n",
            "    Saturninus                     COGNOMEN                  COGNOMEN                  ✅\n",
            "    Laeti f.                       RELATIONSHIP              RELATIONSHIP              ✅\n",
            "    ann. XXV                       AGE                       AGE                       ✅\n",
            "\n",
            "[4] D M Caio Aelio L. f. Firmo tribunus leg X annorum XXXI Antonius Cassianus · fr. faciendum curavit\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    D M                            FORMULA                   FORMULA                   ✅\n",
            "    Caio                           PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Aelio                          NOMEN                     NOMEN                     ✅\n",
            "    L. f.                          RELATIONSHIP              RELATIONSHIP              ✅\n",
            "    Firmo                          COGNOMEN                  COGNOMEN                  ✅\n",
            "    tribunus                       OCCUPATION                OCCUPATION                ✅\n",
            "    leg X                          MILITARY_UNIT             MILITARY_UNIT             ✅\n",
            "    annorum XXXI                   AGE                       AGE                       ✅\n",
            "    Antonius                       NOMEN                     NOMEN                     ✅\n",
            "    Cassianus                      COGNOMEN                  COGNOMEN                  ✅\n",
            "    fr.                            RELATIONSHIP              RELATIONSHIP              ✅\n",
            "\n",
            "[5] Sexto Iulio C. f. Mestrio 7. legio VI Victrix annorum XI Sentius Rusticus con. faciendum curavit\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Sexto                          PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Iulio                          NOMEN                     NOMEN                     ✅\n",
            "    C. f.                          RELATIONSHIP              RELATIONSHIP              ✅\n",
            "    Mestrio                        COGNOMEN                  COGNOMEN                  ✅\n",
            "    7.                             OCCUPATION                OCCUPATION                ✅\n",
            "    legio VI Victrix               MILITARY_UNIT             MILITARY_UNIT             ✅\n",
            "    annorum XI                     AGE                       AGE                       ✅\n",
            "    Sentius                        NOMEN                     NOMEN                     ✅\n",
            "    Rusticus                       COGNOMEN                  COGNOMEN                  ✅\n",
            "    con.                           RELATIONSHIP              RELATIONSHIP              ✅\n",
            "\n",
            "[6] Votum Solvit Libens Merito Proculus Secundi f. vixit annis XIV\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Votum Solvit Libens Merito     FORMULA                   FORMULA                   ✅\n",
            "    Proculus                       COGNOMEN                  COGNOMEN                  ✅\n",
            "    Secundi f.                     RELATIONSHIP              RELATIONSHIP              ✅\n",
            "    vixit annis XIV                AGE                       AGE                       ✅\n",
            "\n",
            "[7] Marcus Valerius Aniensi Fi dus Foro Iuliensium miles legionis X geminae |centuria Vibiani stipendiorum VIIII heres fecit\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Marcus                         PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Valerius                       NOMEN                     NOMEN                     ✅\n",
            "    Aniensi                        -                         TRIBE                     ❌\n",
            "\n",
            "[8] V S L M Laetus Veri f. ann. V\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    V S L M                        FORMULA                   FORMULA                   ✅\n",
            "    Laetus                         COGNOMEN                  COGNOMEN                  ✅\n",
            "    Veri f.                        RELATIONSHIP              RELATIONSHIP              ✅\n",
            "    ann. V                         AGE                       AGE                       ✅\n",
            "\n",
            "[9] Pro Salute Marco Herennio Ter Proculo medicus qui vixit ann. XVI\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Pro Salute                     FORMULA                   FORMULA                   ✅\n",
            "    Marco                          PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Herennio                       NOMEN                     NOMEN                     ✅\n",
            "    Ter                            TRIBE                     TRIBE                     ✅\n",
            "    Proculo                        COGNOMEN                  COGNOMEN                  ✅\n",
            "    medicus                        OCCUPATION                OCCUPATION                ✅\n",
            "    qui vixit ann. XVI             AGE                       AGE                       ✅\n",
            "\n",
            "[10] Iovi Optimo Maximo · Quinto Pompeio Arn Sabino annorum LXXXVI\n",
            "    ENTITY                         PREDICTED                 GOLD                      MATCH\n",
            "    ------------------------------ ------------------------- ------------------------- -----\n",
            "    Iovi Optimo Maximo             FORMULA                   FORMULA                   ✅\n",
            "    Quinto                         PRAENOMEN                 PRAENOMEN                 ✅\n",
            "    Pompeio                        NOMEN                     NOMEN                     ✅\n",
            "    Arn                            ORIGO                     TRIBE                     ❌\n",
            "    Sabino                         COGNOMEN                  COGNOMEN                  ✅\n",
            "    annorum LXXXVI                 AGE                       AGE                       ✅\n",
            "\n",
            "====================================================================================================\n",
            "ACCURACY METRICS\n",
            "====================================================================================================\n",
            "\n",
            "Total Predictions: 4426\n",
            "Total Gold Labels: 5034\n",
            "Correct (Exact Match): 3847\n",
            "\n",
            "📈 Precision: 0.869\n",
            "📈 Recall:    0.764\n",
            "📈 F1 Score:  0.813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ok, deploy the model on real data"
      ],
      "metadata": {
        "id": "Jc8MHpKXGNnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import spacy\n",
        "\n",
        "# The best model should be saved as model-best during training\n",
        "# Load it\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "\n",
        "# Test on real inscriptions from your dataset\n",
        "test_cases = [\n",
        "    \"Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Silvana filio piissimo bene merenti fecit\",\n",
        "    \"Caius Pompeius Caius libertus librarius\",\n",
        "    \"Dis Manibus Caius Pompeius vixit XXXXIII\",\n",
        "    \"Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Quinti fili primi pili legionis XIII\",\n",
        "    \"Figlinae Ocreana\"\n",
        "]\n",
        "\n",
        "print(\"Testing on real inscriptions:\\n\")\n",
        "for text in test_cases:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Text: {text[:70]}...\")\n",
        "    print(\"Entities predicted:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"  {ent.text:25} → {ent.label_:15}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQiq0hK9V8ip",
        "outputId": "3f17c62e-d0e9-4147-9932-fffaf6edb7db"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on real inscriptions:\n",
            "\n",
            "Text: Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Si...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → FORMULA        \n",
            "  Lucio                     → NOMEN          \n",
            "  Ocratio                   → NOMEN          \n",
            "  Corrintho                 → COGNOMEN       \n",
            "  filio                     → RELATIONSHIP   \n",
            "\n",
            "Text: Caius Pompeius Caius libertus librarius...\n",
            "Entities predicted:\n",
            "  Caius                     → PRAENOMEN      \n",
            "\n",
            "Text: Dis Manibus Caius Pompeius vixit XXXXIII...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → FORMULA        \n",
            "  Caius                     → PRAENOMEN      \n",
            "  Pompeius                  → NOMEN          \n",
            "\n",
            "Text: Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Qu...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → FORMULA        \n",
            "  Spediae                   → NOMEN          \n",
            "  Severae                   → COGNOMEN       \n",
            "\n",
            "Text: Figlinae Ocreana...\n",
            "Entities predicted:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data Time\n",
        "\n",
        "Loading in data from P.; should get some from RIB or somewhere."
      ],
      "metadata": {
        "id": "b3bp9U-xyfDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class LeidenProcessor:\n",
        "    \"\"\"Convert Leiden conventions to clean transcription (Corrected)\"\"\"\n",
        "\n",
        "    # Common words that MUST be lowercase for the model to recognize them\n",
        "    # This list covers relationships, age words, and formulas\n",
        "    FORCE_LOWER = {\n",
        "        'filius', 'filia', 'filii', 'filiae', 'f', 'fil',\n",
        "        'vixit', 'v', 'annos', 'annis', 'annorum', 'ann', 'an',\n",
        "        'mensis', 'mensibus', 'm', 'diebus', 'die', 'd',\n",
        "        'hic', 'situs', 'est', 'sita', 'siti', 's', 'h', 'e',\n",
        "        'sit', 'tibi', 'terra', 'levis', 't', 'l',\n",
        "        'bene', 'merenti', 'posuit', 'fecit', 'faciendum', 'curavit',\n",
        "        'pater', 'mater', 'frater', 'soror', 'uxor', 'maritus', 'coniunx',\n",
        "        'de', 'suo', 'et', 'in', 'ex'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def process(leiden_text):\n",
        "        # Step 1: Remove damage markers [3]\n",
        "        text = re.sub(r'\\[\\d+\\]', '', leiden_text)\n",
        "\n",
        "        # Step 2: Remove question marks and simple brackets\n",
        "        text = re.sub(r'\\?', '', text)\n",
        "        text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
        "\n",
        "        # Step 3: Join words broken across lines (Smart Join)\n",
        "        # \"Gem/ellian\" -> \"Gemellian\"\n",
        "        text = re.sub(r'([a-z])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE) # Handle ]/letter\n",
        "\n",
        "        # Step 4: Expand abbreviations (X(expansion) -> Xexpansion)\n",
        "        def expand_abbrev(match):\n",
        "            # If text is \"Q(uintus)\", we just want \"Quintus\"\n",
        "            # If text is \"an(norum)\", we want \"annorum\"\n",
        "            # We concatenate the parts.\n",
        "            # Note: This assumes the Leiden text preserved the case logic.\n",
        "            return match.group(1) + match.group(2)\n",
        "\n",
        "        text = re.sub(r'([A-Za-z]+)\\(([^)]*)\\)', expand_abbrev, text)\n",
        "\n",
        "        # Step 5: Replace remaining slashes with spaces (Line breaks)\n",
        "        text = text.replace('/', ' ')\n",
        "        text = text.replace('\\\\', ' ')\n",
        "\n",
        "        # Step 6: Clean up whitespace and junk\n",
        "        text = re.sub(r'[\\[\\]]', '', text)      # Remove remaining brackets\n",
        "        text = re.sub(r'[<>]', '', text)        # Remove editorial corrections <A=B>\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Step 7: INTELLIGENT CASING (The Fix)\n",
        "        words = text.split()\n",
        "        result = []\n",
        "\n",
        "        for word in words:\n",
        "            # Remove trailing punctuation for checking\n",
        "            clean_word = word.rstrip('.,')\n",
        "\n",
        "            if clean_word.lower() in LeidenProcessor.FORCE_LOWER:\n",
        "                result.append(word.lower())\n",
        "            else:\n",
        "                # Preserve original casing from Leiden!\n",
        "                # Editors usually capitalize names (Iulius) and lowercase verbs (fecit).\n",
        "                # Trust the editor.\n",
        "                result.append(word)\n",
        "\n",
        "        return ' '.join(result)"
      ],
      "metadata": {
        "id": "hRvdZi6Gyg2p"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_PATH = \"training/model-best\"\n",
        "INPUT_CSV = \"assets/actual_inscriptions.csv\"\n",
        "OUTPUT_CSV = \"final_annotated_inscriptions.csv\"\n",
        "\n",
        "# --- 1. LEIDEN PROCESSOR (The Lowercase Logic) ---\n",
        "class LeidenProcessor:\n",
        "    FORCE_LOWER = {\n",
        "        'filius', 'filia', 'filii', 'filiae', 'f', 'fil',\n",
        "        'vixit', 'v', 'annos', 'annis', 'annorum', 'ann', 'an',\n",
        "        'mensis', 'mensibus', 'm', 'diebus', 'die', 'd',\n",
        "        'hic', 'situs', 'est', 'sita', 'siti', 's', 'h', 'e',\n",
        "        'sit', 'tibi', 'terra', 'levis', 't', 'l',\n",
        "        'bene', 'merenti', 'posuit', 'fecit', 'faciendum', 'curavit',\n",
        "        'pater', 'mater', 'frater', 'soror', 'uxor', 'maritus', 'coniunx',\n",
        "        'de', 'suo', 'et', 'in', 'ex', 'sacrum', 'dis', 'manibus'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def process(leiden_text):\n",
        "        if not isinstance(leiden_text, str): return \"\"\n",
        "        text = re.sub(r'\\[\\d+\\]', '', leiden_text) # Remove damage [3]\n",
        "        text = re.sub(r'\\?', '', text)\n",
        "        text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text) # Flatten brackets\n",
        "        text = re.sub(r'([a-z])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE) # Join lines\n",
        "\n",
        "        # Expand abbreviations X(yz) -> Xyz\n",
        "        text = re.sub(r'([A-Za-z]+)\\(([^)]*)\\)', lambda m: m.group(1)+m.group(2), text)\n",
        "\n",
        "        text = text.replace('/', ' ').replace('\\\\', ' ')\n",
        "        text = re.sub(r'[\\[\\]<>]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Intelligent Casing\n",
        "        words = text.split()\n",
        "        result = []\n",
        "        for word in words:\n",
        "            clean = word.rstrip('.,')\n",
        "            if clean.lower() in LeidenProcessor.FORCE_LOWER:\n",
        "                result.append(word.lower())\n",
        "            else:\n",
        "                result.append(word) # Keep original casing for names\n",
        "        return ' '.join(result)\n",
        "\n",
        "# --- 2. LOAD RESOURCES ---\n",
        "print(\"Loading model...\")\n",
        "nlp = spacy.load(MODEL_PATH)\n",
        "\n",
        "# Regex to catch ages that might be lowercased (e.g., 'annorum l')\n",
        "# Matches: annorum/vixit + space + (roman numerals OR digits)\n",
        "age_regex = re.compile(r'(?i)\\b(annorum|annis|ann|an|vixit|v)\\W+(\\d+|[IVXLCDM]+|[lxiv]+)\\b')\n",
        "\n",
        "# --- 3. PROCESSING LOOP ---\n",
        "print(f\"Processing {INPUT_CSV}...\")\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "records = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    raw_text = row.get('text', '')\n",
        "\n",
        "    # A. Clean Text\n",
        "    clean_text = LeidenProcessor.process(raw_text)\n",
        "    if not clean_text: continue\n",
        "\n",
        "    # B. AI Prediction\n",
        "    doc = nlp(clean_text)\n",
        "\n",
        "    preds = {}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ not in preds: preds[ent.label_] = []\n",
        "        preds[ent.label_].append(ent.text)\n",
        "\n",
        "    # C. Regex Safety Net (Fixes the \"lowercase l\" issue)\n",
        "    if 'AGE' not in preds:\n",
        "        match = age_regex.search(clean_text)\n",
        "        if match:\n",
        "            preds['AGE'] = [match.group(0)]\n",
        "\n",
        "    # D. Prepare Output\n",
        "    row_out = {\n",
        "        \"id\": row.get('id', f\"row_{idx}\"),\n",
        "        \"text_clean\": clean_text,\n",
        "        \"text_raw\": raw_text,\n",
        "        \"entity_count\": len(doc.ents) + (1 if 'AGE' in preds and 'AGE' not in [e.label_ for e in doc.ents] else 0)\n",
        "    }\n",
        "\n",
        "    # Flatten entities to string\n",
        "    labels_of_interest = [\n",
        "        'PRAENOMEN', 'NOMEN', 'COGNOMEN', 'TRIBE', 'ORIGO',\n",
        "        'FORMULA', 'OCCUPATION', 'MILITARY_UNIT', 'AGE', 'RELATIONSHIP'\n",
        "    ]\n",
        "\n",
        "    for label in labels_of_interest:\n",
        "        row_out[label] = \" | \".join(preds.get(label, []))\n",
        "\n",
        "    records.append(row_out)\n",
        "\n",
        "# --- 4. SAVE ---\n",
        "out_df = pd.DataFrame(records)\n",
        "out_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"✅ Success! Processed {len(records)} rows.\")\n",
        "print(out_df[['text_clean', 'PRAENOMEN', 'NOMEN', 'COGNOMEN', 'AGE']].head(10).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06RYzARzaAXs",
        "outputId": "9ddc9a8f-f375-4032-c94b-52f5bac10e98"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Processing assets/actual_inscriptions.csv...\n",
            "✅ Success! Processed 138 rows.\n",
            "                                                                                text_clean PRAENOMEN    NOMEN    COGNOMEN          AGE\n",
            "0                   Iovi Optimo Maximo Quintus Cassius Cassianus animo libens votum posuit   Quintus  Cassius   Cassianus             \n",
            "1      dis manibus sacrum Sycecale vixit anno mensibus v sorores Tricisma Salcea et Vegeta                                            \n",
            "2                                                                       Figlina Gemelliana                                            \n",
            "3  dis manibus sacrum Severus Tongini annorum XXI hic situs est sit tibi terra levis mater                        Severus  annorum XXI\n",
            "4                                 Iovi Optimo Maximo Caius As Ms votum solvit libens animo     Caius                                  \n",
            "5                     Turaesio Turcaudi filius et Pans Maeilonis filii sit eis terra levis                      Maeilonis             \n",
            "6                                                                     Caeno Loucini filius                          Caeno             \n",
            "7                                                          Saturninus Bouti filius annorum                     Saturninus             \n",
            "8                  annorum XVI hic situs est sit tibi terra levis Afelia avia de suo fecit                                 annorum XVI\n",
            "9                                   C N Papiria annorum l hic situs est tibi terra levis v         N                         annorum l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r inscription_model.zip training/model-best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zYZrb4fQ8A4I",
        "outputId": "fef48b78-c6a1-4368-aeb7-36d2f00df138"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: training/model-best/ (stored 0%)\n",
            "  adding: training/model-best/meta.json (deflated 64%)\n",
            "  adding: training/model-best/tok2vec/ (stored 0%)\n",
            "  adding: training/model-best/tok2vec/cfg (stored 0%)\n",
            "  adding: training/model-best/tok2vec/model (deflated 7%)\n",
            "  adding: training/model-best/tokenizer (deflated 82%)\n",
            "  adding: training/model-best/config.cfg (deflated 60%)\n",
            "  adding: training/model-best/ner/ (stored 0%)\n",
            "  adding: training/model-best/ner/cfg (deflated 33%)\n",
            "  adding: training/model-best/ner/moves (deflated 72%)\n",
            "  adding: training/model-best/ner/model (deflated 7%)\n",
            "  adding: training/model-best/vocab/ (stored 0%)\n",
            "  adding: training/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: training/model-best/vocab/strings.json (deflated 82%)\n",
            "  adding: training/model-best/vocab/key2row (stored 0%)\n",
            "  adding: training/model-best/vocab/vectors (deflated 10%)\n",
            "  adding: training/model-best/vocab/vectors.cfg (deflated 28%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And Now We Try The LATINEPI Thing\n",
        "\n",
        "Which was initially supposed to be a ML-free approach to extracting structured data. It uses a whole bunch of pattern matching, but now also will use the trained underlying latinCy model we built above _first_ then the other stuff. Worth a shot... (and I should put some more effort into the patterns maybe)."
      ],
      "metadata": {
        "id": "d--rdp2nRXTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone fresh\n",
        "!git clone https://github.com/shawngraham/latinepi.git\n",
        "%cd latinepi\n",
        "!pip install -e .\n",
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ruw4DLzSRdk9",
        "outputId": "8bd54350-dea5-43e0-e9db-54c5634d945b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'latinepi'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Total 389 (delta 0), reused 0 (delta 0), pack-reused 389 (from 1)\u001b[K\n",
            "Receiving objects: 100% (389/389), 941.91 KiB | 2.03 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n",
            "/content/latinepi\n",
            "Obtaining file:///content/latinepi\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from latinepi==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from latinepi==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->latinepi==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->latinepi==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->latinepi==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->latinepi==0.1.0) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->latinepi==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->latinepi==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->latinepi==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->latinepi==0.1.0) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->latinepi==0.1.0) (1.17.0)\n",
            "Building wheels for collected packages: latinepi\n",
            "  Building editable for latinepi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for latinepi: filename=latinepi-0.1.0-0.editable-py3-none-any.whl size=10358 sha256=0613d9e152c40a841b66ce470616a508d84c23eaab188c5253c24df8e21e20d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rydskdk9/wheels/27/49/bd/699880d166b00e5fc8881fe68f1b9364f78c0a2dde49d64999\n",
            "Successfully built latinepi\n",
            "Installing collected packages: latinepi\n",
            "Successfully installed latinepi-0.1.0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!latinepi --input assets/actual_inscriptions.csv --output results.json --use-ml-hybrid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TBmkcyyaZ7Sf",
        "outputId": "038bc244-68c7-4ce6-de7a-37273f82dadb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/latinepi/latinepi/grammar_patterns.py:291: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  'BENE\\s+MERENTI': ('well-deserving', 0.75),\n",
            "Using ML+regex hybrid extractor with:\n",
            "  - Fine-tuned latinCy spaCy model (primary)\n",
            "  - Regex pattern validation (confidence boosting)\n",
            "Processing 138 inscription(s)...\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 1/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 2/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 3/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 4/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 5/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 6/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 7/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 8/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 9/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 10/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 11/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 12/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 13/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 14/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 15/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 16/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 17/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 18/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 19/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 20/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 21/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 22/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 23/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 24/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 25/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 26/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 27/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 28/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 29/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 30/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 31/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 32/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 33/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 34/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 35/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 36/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 37/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 38/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 39/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 40/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 41/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 42/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 43/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 44/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 45/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 46/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 47/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 48/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 49/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 50/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 51/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 52/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 53/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 54/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 55/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 56/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 57/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 58/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 59/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 60/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 61/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 62/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 63/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 64/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 65/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 66/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 67/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 68/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 69/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 70/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 71/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 72/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 73/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 74/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 75/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 76/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 77/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 78/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 79/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 80/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 81/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 82/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 83/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 84/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 85/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 86/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 87/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 88/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 89/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 90/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 91/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 92/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 93/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 94/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 95/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 96/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 97/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 98/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 99/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 100/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 101/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 102/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 103/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 104/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 105/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 106/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 107/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 108/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 109/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 110/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 111/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 112/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 113/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 114/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 115/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 116/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 117/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 118/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 119/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 120/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 121/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 122/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 123/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 124/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 125/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 126/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 127/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 128/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 129/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 130/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 131/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 132/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 133/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 134/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 135/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 136/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 137/138\n",
            "✅ Loaded latinCy model from training/model-best\n",
            "Processed inscription 138/138\n",
            "Successfully processed 138 inscription(s) -> 'results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/latinepi')\n",
        "\n",
        "# Import directly from the module file\n",
        "from latinepi.hybrid_ml_extractor import extract_entities_hybrid as extract_entities_ml_hybrid\n",
        "\n",
        "text = \"D(is) M(anibus) // P(ublius) Ael(ius) P(ubli) f(ilius) Maecia / Mestrius Pela(gonia) / opt(io) leg(ionis) II ad(iutricis) / |(centuria) Attei Dextri / an(norum) XXXVIIII stip(endiorum) / XVIIII h(ic) s(itus) e(st) opti/ones leg(ionis) eiusde(m) / f(aciendum) [c(uraverunt)]\"\n",
        "entities = extract_entities_ml_hybrid(text)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z04owV5db8NZ",
        "outputId": "c18e6be8-ab55-46b6-8ef4-2981e19b0059"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded latinCy model from training/model-best\n",
            "{'cognomen': {'value': 'Mestrius', 'confidence': 0.85}, 'years_lived': {'value': '39', 'confidence': 0.85}, 'military_service': {'value': 'Miles', 'confidence': 0.75}, 'tribe': {'value': 'Maecia', 'confidence': 0.88}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!latinepi --input assets/actual_inscriptions.csv --output results.json --use-ml-hybrid"
      ],
      "metadata": {
        "id": "WS4JgbEKdWUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the end."
      ],
      "metadata": {
        "id": "sZXjD1S2hwWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## some debugging\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def check_label_counts(path):\n",
        "    c = Counter()\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            rec = json.loads(line)\n",
        "            for ent in rec['annotations']:\n",
        "                c[ent[2]] += 1\n",
        "    print(f\"Stats for {path}:\")\n",
        "    print(c.most_common())\n",
        "\n",
        "check_label_counts('assets/train.jsonl')\n",
        "check_label_counts('assets/dev.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtLki7JrpTYH",
        "outputId": "b95ae059-0696-4db5-b47a-71503a28a5f9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for assets/train.jsonl:\n",
            "[('COGNOMEN', 3750), ('NOMEN', 3297), ('FORMULA', 2942), ('RELATIONSHIP', 2397), ('PRAENOMEN', 2217), ('AGE', 1615), ('OCCUPATION', 1171), ('TRIBE', 851), ('MILITARY_UNIT', 680), ('ORIGO', 473)]\n",
            "Stats for assets/dev.jsonl:\n",
            "[('COGNOMEN', 939), ('NOMEN', 841), ('FORMULA', 812), ('RELATIONSHIP', 622), ('PRAENOMEN', 560), ('AGE', 395), ('OCCUPATION', 281), ('TRIBE', 186), ('MILITARY_UNIT', 173), ('ORIGO', 105)]\n"
          ]
        }
      ]
    }
  ]
}